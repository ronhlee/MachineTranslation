{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MachineTranslationWithAttention.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"ySw0xMWra1TH","colab_type":"text"},"cell_type":"markdown","source":["# Neural Machine Translation\n","\n","Build a Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\") ... using an attention model. "]},{"metadata":{"id":"0iQvA-iNVV1t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"a46af5b3-c570-42d5-e811-a370b385e5fc","executionInfo":{"status":"ok","timestamp":1542141464507,"user_tz":480,"elapsed":41475,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"7qzSp6uXGLhx","colab_type":"code","colab":{}},"cell_type":"code","source":["!cp -R gdrive/My\\ Drive/Colab\\ Notebooks/DeepLearning.ai/W3MachineTranslation/* ."],"execution_count":0,"outputs":[]},{"metadata":{"id":"ILzuUGuOGdW7","colab_type":"code","colab":{}},"cell_type":"code","source":["# !pip install faker\n","# !pip install babel"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0AF8cjYUa1TL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6973569a-375b-4961-ad05-ad37dcc7093f","executionInfo":{"status":"ok","timestamp":1542142651072,"user_tz":480,"elapsed":2004,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","from keras.layers import RepeatVector, Dense, Activation, Lambda\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","from keras.models import load_model, Model\n","import keras.backend as K\n","import numpy as np\n","\n","from faker import Faker\n","import random\n","from tqdm import tqdm\n","from babel.dates import format_date\n","from nmt_utils import *\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"4ruKMYrca1TZ","colab_type":"text"},"cell_type":"markdown","source":["## 1 - Translating human readable dates into machine readable dates"]},{"metadata":{"id":"oY7kG1ONa1Tb","colab_type":"text"},"cell_type":"markdown","source":["### 1.1 - Dataset\n","\n","10000 human readable dates and their equivalent, standardized, machine readable dates."]},{"metadata":{"id":"vaADZsq4a1Td","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"af26ec38-ad5a-4325-979e-402d73a616f4","executionInfo":{"status":"ok","timestamp":1542142656004,"user_tz":480,"elapsed":1097,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["m = 10000\n","dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["100%|██████████| 10000/10000 [00:00<00:00, 19303.42it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"IJ1rZ7Mta1Tk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"68922adf-8e27-4097-f4c5-783b8f8955cf","executionInfo":{"status":"ok","timestamp":1542142657600,"user_tz":480,"elapsed":888,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["dataset[10:20]"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('friday june 16 1978', '1978-06-16'),\n"," ('friday june 18 1999', '1999-06-18'),\n"," ('8 oct 1987', '1987-10-08'),\n"," ('tuesday april 1 2008', '2008-04-01'),\n"," ('6 september 2012', '2012-09-06'),\n"," ('12 november 1981', '1981-11-12'),\n"," ('friday august 19 2011', '2011-08-19'),\n"," ('25 08 07', '2007-08-25'),\n"," ('18 september 1982', '1982-09-18'),\n"," ('friday february 25 1994', '1994-02-25')]"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"fA1Oszpna1Tr","colab_type":"text"},"cell_type":"markdown","source":["- `dataset`: a list of tuples of (human readable date, machine readable date)\n","- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index \n","- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with `human_vocab`. \n","- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n","\n","Tx is the number of max characters in human readable dates. If longer, will be truncated.\n","Ty is the number of max characters in machine readable dates"]},{"metadata":{"id":"cJVtthsBa1Tt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"903aadd8-902c-4c6f-89fc-f70d581f0477","executionInfo":{"status":"ok","timestamp":1542142661912,"user_tz":480,"elapsed":1154,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["Tx = 30\n","Ty = 10\n","X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n","\n","print(\"X.shape:\", X.shape)\n","print(\"Y.shape:\", Y.shape)\n","print(\"Xoh.shape:\", Xoh.shape)\n","print(\"Yoh.shape:\", Yoh.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["X.shape: (10000, 30)\n","Y.shape: (10000, 10)\n","Xoh.shape: (10000, 30, 37)\n","Yoh.shape: (10000, 10, 11)\n"],"name":"stdout"}]},{"metadata":{"id":"elVtOsYna1Tz","colab_type":"text"},"cell_type":"markdown","source":["- `X`: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via `human_vocab`. Each date is further padded to $T_x$ values with a special character (< pad >). `X.shape = (m, Tx)`\n","- `Y`: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in `machine_vocab`. You should have `Y.shape = (m, Ty)`. \n","- `Xoh`: one-hot version of `X`, the \"1\" entry's index is mapped to the character thanks to `human_vocab`. `Xoh.shape = (m, Tx, len(human_vocab))`\n","- `Yoh`: one-hot version of `Y`, the \"1\" entry's index is mapped to the character thanks to `machine_vocab`. `Yoh.shape = (m, Tx, len(machine_vocab))`. Here, `len(machine_vocab) = 11` since there are 11 characters ('-' as well as 0-9). \n"]},{"metadata":{"id":"ennJrS7Aa1T1","colab_type":"text"},"cell_type":"markdown","source":["Visualize Pre-processing"]},{"metadata":{"id":"6pfaBDNna1T4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"71de73b9-6347-4651-f219-7619b7e9c108","executionInfo":{"status":"ok","timestamp":1542142664131,"user_tz":480,"elapsed":871,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["index = 0\n","print(\"Source date:\", dataset[index][0])\n","print(\"Target date:\", dataset[index][1])\n","print()\n","print(\"Source after preprocessing (indices):\", X[index])\n","print(\"Target after preprocessing (indices):\", Y[index])\n","print()\n","print(\"Source after preprocessing (one-hot):\", Xoh[index])\n","print(\"Target after preprocessing (one-hot):\", Yoh[index])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Source date: 9 may 1998\n","Target date: 1998-05-09\n","\n","Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n"," 36 36 36 36 36 36]\n","Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n","\n","Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]]\n","Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"],"name":"stdout"}]},{"metadata":{"id":"_S4Ycliua1T9","colab_type":"text"},"cell_type":"markdown","source":["## 2 - Neural machine translation with attention\n","\n"]},{"metadata":{"id":"H0w_ebmBa1T_","colab_type":"text"},"cell_type":"markdown","source":["\n"," [RepeatVector()](https://keras.io/layers/core/#repeatvector), [Concatenate()](https://keras.io/layers/merge/#concatenate), [Dense()](https://keras.io/layers/core/#dense), [Activation()](https://keras.io/layers/core/#activation), [Dot()](https://keras.io/layers/merge/#dot)."]},{"metadata":{"id":"SZDD67N8a1UD","colab_type":"code","colab":{}},"cell_type":"code","source":["# Defined shared layers as global variables\n","repeator = RepeatVector(Tx)\n","concatenator = Concatenate(axis=-1)\n","densor1 = Dense(10, activation = \"tanh\")\n","densor2 = Dense(1, activation = \"relu\")\n","activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n","dotor = Dot(axes = 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7BFA_gEma1UJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# one_step_attention\n","\n","def one_step_attention(a, s_prev):\n","    \"\"\"\n","    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n","    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n","    \n","    Arguments:\n","    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n","    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n","    \n","    Returns:\n","    context -- context vector, input of the next (post-attetion) LSTM cell\n","    \"\"\"\n","    \n","    \n","    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n","    s_prev = repeator(s_prev)\n","    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n","    concat = concatenator([a, s_prev])\n","    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n","    e = densor1(concat)\n","    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n","    energies = densor2(e)\n","    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n","    alphas = activator(energies)\n","    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n","    context = dotor([alphas, a])\n","    \n","    \n","    return context"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j6dFNGgXa1UQ","colab_type":"text"},"cell_type":"markdown","source":["Implement `model()`. Global Layers defined above share weights"]},{"metadata":{"id":"wIDJ0LWua1US","colab_type":"code","colab":{}},"cell_type":"code","source":["n_a = 32\n","n_s = 64\n","post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n","output_layer = Dense(len(machine_vocab), activation=softmax)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MLWbvQFha1UY","colab_type":"code","colab":{}},"cell_type":"code","source":["# model\n","\n","def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n","    \"\"\"\n","    Arguments:\n","    Tx -- length of the input sequence\n","    Ty -- length of the output sequence\n","    n_a -- hidden state size of the Bi-LSTM\n","    n_s -- hidden state size of the post-attention LSTM\n","    human_vocab_size -- size of the python dictionary \"human_vocab\"\n","    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n","\n","    Returns:\n","    model -- Keras model instance\n","    \"\"\"\n","    \n","    # Define the inputs of your model with a shape (Tx,)\n","    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n","    X = Input(shape=(Tx, human_vocab_size))\n","    s0 = Input(shape=(n_s,), name='s0')\n","    c0 = Input(shape=(n_s,), name='c0')\n","    s = s0\n","    c = c0\n","    \n","    # Initialize empty list of outputs\n","    outputs = []\n","    \n","       \n","    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n","    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n","    \n","    # Step 2: Iterate for Ty steps\n","    for t in range(Ty):\n","    \n","        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n","        context = one_step_attention(a, s)\n","        \n","        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n","        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n","        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n","        \n","        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n","        out = output_layer(s)\n","        \n","        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n","        outputs.append(out)\n","    \n","    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n","    model = Model((X,s0,c0), outputs)\n","    \n","    \n","    \n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8AHItq1Xa1Ud","colab_type":"text"},"cell_type":"markdown","source":["Create the model."]},{"metadata":{"id":"igyrr75ea1Ug","colab_type":"code","colab":{}},"cell_type":"code","source":["model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FyI7Ga-ya1Ul","colab_type":"text"},"cell_type":"markdown","source":["A summary of the model..."]},{"metadata":{"id":"OQkut1d-a1Uo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2448},"outputId":"022e7a65-dd76-449f-d712-958db2fb5774","executionInfo":{"status":"ok","timestamp":1542142689206,"user_tz":480,"elapsed":1032,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["model.summary()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 30, 37)       0                                            \n","__________________________________________________________________________________________________\n","s0 (InputLayer)                 (None, 64)           0                                            \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n","                                                                 lstm_1[0][0]                     \n","                                                                 lstm_1[1][0]                     \n","                                                                 lstm_1[2][0]                     \n","                                                                 lstm_1[3][0]                     \n","                                                                 lstm_1[4][0]                     \n","                                                                 lstm_1[5][0]                     \n","                                                                 lstm_1[6][0]                     \n","                                                                 lstm_1[7][0]                     \n","                                                                 lstm_1[8][0]                     \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[0][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[1][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[2][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[3][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[4][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[5][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[6][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[7][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[8][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[9][0]            \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n","                                                                 concatenate_1[1][0]              \n","                                                                 concatenate_1[2][0]              \n","                                                                 concatenate_1[3][0]              \n","                                                                 concatenate_1[4][0]              \n","                                                                 concatenate_1[5][0]              \n","                                                                 concatenate_1[6][0]              \n","                                                                 concatenate_1[7][0]              \n","                                                                 concatenate_1[8][0]              \n","                                                                 concatenate_1[9][0]              \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n","                                                                 dense_1[1][0]                    \n","                                                                 dense_1[2][0]                    \n","                                                                 dense_1[3][0]                    \n","                                                                 dense_1[4][0]                    \n","                                                                 dense_1[5][0]                    \n","                                                                 dense_1[6][0]                    \n","                                                                 dense_1[7][0]                    \n","                                                                 dense_1[8][0]                    \n","                                                                 dense_1[9][0]                    \n","__________________________________________________________________________________________________\n","attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n","                                                                 dense_2[1][0]                    \n","                                                                 dense_2[2][0]                    \n","                                                                 dense_2[3][0]                    \n","                                                                 dense_2[4][0]                    \n","                                                                 dense_2[5][0]                    \n","                                                                 dense_2[6][0]                    \n","                                                                 dense_2[7][0]                    \n","                                                                 dense_2[8][0]                    \n","                                                                 dense_2[9][0]                    \n","__________________________________________________________________________________________________\n","dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[1][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[2][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[3][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[4][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[5][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[6][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[7][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[8][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[9][0]          \n","                                                                 bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","c0 (InputLayer)                 (None, 64)           0                                            \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n","                                                                 s0[0][0]                         \n","                                                                 c0[0][0]                         \n","                                                                 dot_1[1][0]                      \n","                                                                 lstm_1[0][0]                     \n","                                                                 lstm_1[0][2]                     \n","                                                                 dot_1[2][0]                      \n","                                                                 lstm_1[1][0]                     \n","                                                                 lstm_1[1][2]                     \n","                                                                 dot_1[3][0]                      \n","                                                                 lstm_1[2][0]                     \n","                                                                 lstm_1[2][2]                     \n","                                                                 dot_1[4][0]                      \n","                                                                 lstm_1[3][0]                     \n","                                                                 lstm_1[3][2]                     \n","                                                                 dot_1[5][0]                      \n","                                                                 lstm_1[4][0]                     \n","                                                                 lstm_1[4][2]                     \n","                                                                 dot_1[6][0]                      \n","                                                                 lstm_1[5][0]                     \n","                                                                 lstm_1[5][2]                     \n","                                                                 dot_1[7][0]                      \n","                                                                 lstm_1[6][0]                     \n","                                                                 lstm_1[6][2]                     \n","                                                                 dot_1[8][0]                      \n","                                                                 lstm_1[7][0]                     \n","                                                                 lstm_1[7][2]                     \n","                                                                 dot_1[9][0]                      \n","                                                                 lstm_1[8][0]                     \n","                                                                 lstm_1[8][2]                     \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n","                                                                 lstm_1[1][0]                     \n","                                                                 lstm_1[2][0]                     \n","                                                                 lstm_1[3][0]                     \n","                                                                 lstm_1[4][0]                     \n","                                                                 lstm_1[5][0]                     \n","                                                                 lstm_1[6][0]                     \n","                                                                 lstm_1[7][0]                     \n","                                                                 lstm_1[8][0]                     \n","                                                                 lstm_1[9][0]                     \n","==================================================================================================\n","Total params: 52,960\n","Trainable params: 52,960\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"bJjO0DI_a1U3","colab_type":"text"},"cell_type":"markdown","source":["Compile the model in Keras. \n","\n","Loss : `categorical_crossentropy` loss\n","\n","Optimizer : a custom [Adam](https://keras.io/optimizers/#adam) [optimizer](https://keras.io/optimizers/#usage-of-optimizers) (`learning rate = 0.005`, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, `decay = 0.01`)\n","\n","Metrics : accuracy"]},{"metadata":{"id":"pv9jOY99a1U5","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","opt = model.compile(optimizer=Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n","opt\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yY5hfacsa1U_","colab_type":"text"},"cell_type":"markdown","source":["Define all the inputs and outputs to fit the model:\n","- X of shape $(m = 10000, T_x = 30)$ containing the training examples.\n","- `s0` and `c0` to initialize the `post_activation_LSTM_cell` with 0s.\n","- Given the `model()`, \"outputs\" need to be a list of 11 elements of shape (m, T_y). So that: `outputs[i][0], ..., outputs[i][Ty]` represent the true labels (characters) corresponding to the $i^{th}$ training example (`X[i]`). More generally, `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."]},{"metadata":{"id":"bES2C9Gea1VA","colab_type":"code","colab":{}},"cell_type":"code","source":["s0 = np.zeros((m, n_s))\n","c0 = np.zeros((m, n_s))\n","outputs = list(Yoh.swapaxes(0,1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E2aechjka1VF","colab_type":"text"},"cell_type":"markdown","source":["Let's now fit the model and run it for one epoch."]},{"metadata":{"id":"Fx8Z8KaHa1VH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"c0308e63-74b1-405d-f913-4a8a3fe76658","executionInfo":{"status":"ok","timestamp":1542142735603,"user_tz":480,"elapsed":35260,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch 1/1\n","10000/10000 [==============================] - 24s 2ms/step - loss: 16.9152 - dense_3_loss: 2.5260 - dense_3_acc: 0.4199 - dense_3_acc_1: 0.7001 - dense_3_acc_2: 0.3269 - dense_3_acc_3: 0.0806 - dense_3_acc_4: 0.9066 - dense_3_acc_5: 0.3100 - dense_3_acc_6: 0.0493 - dense_3_acc_7: 0.9176 - dense_3_acc_8: 0.2659 - dense_3_acc_9: 0.1104\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fa5de591048>"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"xpDcpyU9a1VO","colab_type":"text"},"cell_type":"markdown","source":["`dense_2_acc_8: 0.89` means that the 7th character of the output is predicted correctly 89% of the time in the current batch of data."]},{"metadata":{"id":"J-43FC7Pa1VQ","colab_type":"code","colab":{}},"cell_type":"code","source":["model.load_weights('models/model.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZuUV2kOS4Ry4","colab_type":"text"},"cell_type":"markdown","source":["Test some examples..."]},{"metadata":{"id":"q0xFdrkca1VW","colab_type":"code","colab":{}},"cell_type":"code","source":["EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n","for example in EXAMPLES:\n","    \n","    source = string_to_int(example, Tx, human_vocab)\n","    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n","    prediction = model.predict([source, s0, c0])\n","    prediction = np.argmax(prediction, axis = -1)\n","    output = [inv_machine_vocab[int(i)] for i in prediction]\n","    \n","    print(\"source:\", example)\n","    print(\"output:\", ''.join(output))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sA2uieYna1Ve","colab_type":"text"},"cell_type":"markdown","source":["## 3 - Visualizing Attention\n","\n","\n"]},{"metadata":{"id":"5Rk1MznHa1Vh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2448},"outputId":"21f8ce44-f421-4224-e1de-2d16be0ad27d","executionInfo":{"status":"ok","timestamp":1542143153642,"user_tz":480,"elapsed":789,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["model.summary()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 30, 37)       0                                            \n","__________________________________________________________________________________________________\n","s0 (InputLayer)                 (None, 64)           0                                            \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n","                                                                 lstm_1[0][0]                     \n","                                                                 lstm_1[1][0]                     \n","                                                                 lstm_1[2][0]                     \n","                                                                 lstm_1[3][0]                     \n","                                                                 lstm_1[4][0]                     \n","                                                                 lstm_1[5][0]                     \n","                                                                 lstm_1[6][0]                     \n","                                                                 lstm_1[7][0]                     \n","                                                                 lstm_1[8][0]                     \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[0][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[1][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[2][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[3][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[4][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[5][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[6][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[7][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[8][0]            \n","                                                                 bidirectional_1[0][0]            \n","                                                                 repeat_vector_1[9][0]            \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n","                                                                 concatenate_1[1][0]              \n","                                                                 concatenate_1[2][0]              \n","                                                                 concatenate_1[3][0]              \n","                                                                 concatenate_1[4][0]              \n","                                                                 concatenate_1[5][0]              \n","                                                                 concatenate_1[6][0]              \n","                                                                 concatenate_1[7][0]              \n","                                                                 concatenate_1[8][0]              \n","                                                                 concatenate_1[9][0]              \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n","                                                                 dense_1[1][0]                    \n","                                                                 dense_1[2][0]                    \n","                                                                 dense_1[3][0]                    \n","                                                                 dense_1[4][0]                    \n","                                                                 dense_1[5][0]                    \n","                                                                 dense_1[6][0]                    \n","                                                                 dense_1[7][0]                    \n","                                                                 dense_1[8][0]                    \n","                                                                 dense_1[9][0]                    \n","__________________________________________________________________________________________________\n","attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n","                                                                 dense_2[1][0]                    \n","                                                                 dense_2[2][0]                    \n","                                                                 dense_2[3][0]                    \n","                                                                 dense_2[4][0]                    \n","                                                                 dense_2[5][0]                    \n","                                                                 dense_2[6][0]                    \n","                                                                 dense_2[7][0]                    \n","                                                                 dense_2[8][0]                    \n","                                                                 dense_2[9][0]                    \n","__________________________________________________________________________________________________\n","dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[1][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[2][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[3][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[4][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[5][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[6][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[7][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[8][0]          \n","                                                                 bidirectional_1[0][0]            \n","                                                                 attention_weights[9][0]          \n","                                                                 bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","c0 (InputLayer)                 (None, 64)           0                                            \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n","                                                                 s0[0][0]                         \n","                                                                 c0[0][0]                         \n","                                                                 dot_1[1][0]                      \n","                                                                 lstm_1[0][0]                     \n","                                                                 lstm_1[0][2]                     \n","                                                                 dot_1[2][0]                      \n","                                                                 lstm_1[1][0]                     \n","                                                                 lstm_1[1][2]                     \n","                                                                 dot_1[3][0]                      \n","                                                                 lstm_1[2][0]                     \n","                                                                 lstm_1[2][2]                     \n","                                                                 dot_1[4][0]                      \n","                                                                 lstm_1[3][0]                     \n","                                                                 lstm_1[3][2]                     \n","                                                                 dot_1[5][0]                      \n","                                                                 lstm_1[4][0]                     \n","                                                                 lstm_1[4][2]                     \n","                                                                 dot_1[6][0]                      \n","                                                                 lstm_1[5][0]                     \n","                                                                 lstm_1[5][2]                     \n","                                                                 dot_1[7][0]                      \n","                                                                 lstm_1[6][0]                     \n","                                                                 lstm_1[6][2]                     \n","                                                                 dot_1[8][0]                      \n","                                                                 lstm_1[7][0]                     \n","                                                                 lstm_1[7][2]                     \n","                                                                 dot_1[9][0]                      \n","                                                                 lstm_1[8][0]                     \n","                                                                 lstm_1[8][2]                     \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n","                                                                 lstm_1[1][0]                     \n","                                                                 lstm_1[2][0]                     \n","                                                                 lstm_1[3][0]                     \n","                                                                 lstm_1[4][0]                     \n","                                                                 lstm_1[5][0]                     \n","                                                                 lstm_1[6][0]                     \n","                                                                 lstm_1[7][0]                     \n","                                                                 lstm_1[8][0]                     \n","                                                                 lstm_1[9][0]                     \n","==================================================================================================\n","Total params: 52,960\n","Trainable params: 52,960\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"7BFVUzh7a1Vo","colab_type":"text"},"cell_type":"markdown","source":["the layer named `attention_weights` outputs the `alphas` of shape (m, 30, 1) before `dot_2` computes the context vector for every time step $t = 0, \\ldots, T_y-1$. \n","\n","The function `attention_map()` pulls out the attention values from that layer and plots them."]},{"metadata":{"scrolled":false,"id":"UFgp8Bzta1Vq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":459},"outputId":"8ab1ba26-ddf4-423f-cb2d-199aca39b799","executionInfo":{"status":"ok","timestamp":1542143374603,"user_tz":480,"elapsed":3489,"user":{"displayName":"Ron Lee","photoUrl":"","userId":"08160863395854553252"}}},"cell_type":"code","source":["attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<matplotlib.figure.Figure at 0x7fa5d3bff278>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAecAAAGpCAYAAAC3X/5IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7fa5d3bff438>"]},"metadata":{"tags":[]}}]}]}